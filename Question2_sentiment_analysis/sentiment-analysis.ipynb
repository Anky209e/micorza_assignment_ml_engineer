{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d53dd5",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "We use the `IMDB movie_review` dataset from NLTK to build:\n",
    "- A baseline model using TF-IDF + SVM\n",
    "- Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576ca7f",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8951650a",
   "metadata": {},
   "source": [
    "#### I trained it on kaggle so this is code for accesing files on kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f224d8-ccd5-445f-99d9-a48d7463d03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:55:07.192681Z",
     "iopub.status.busy": "2025-04-19T15:55:07.192505Z",
     "iopub.status.idle": "2025-04-19T15:55:08.811795Z",
     "shell.execute_reply": "2025-04-19T15:55:08.811055Z",
     "shell.execute_reply.started": "2025-04-19T15:55:07.192664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43006259",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:55:22.382232Z",
     "iopub.status.busy": "2025-04-19T15:55:22.381881Z",
     "iopub.status.idle": "2025-04-19T15:55:48.316087Z",
     "shell.execute_reply": "2025-04-19T15:55:48.315506Z",
     "shell.execute_reply.started": "2025-04-19T15:55:22.382209Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 15:55:35.622644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745078135.804346      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745078135.859881      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer,TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da7bfad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:56:00.114019Z",
     "iopub.status.busy": "2025-04-19T15:56:00.113455Z",
     "iopub.status.idle": "2025-04-19T15:56:00.236596Z",
     "shell.execute_reply": "2025-04-19T15:56:00.236061Z",
     "shell.execute_reply.started": "2025-04-19T15:56:00.113976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020aeba6",
   "metadata": {},
   "source": [
    "### Functions for cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb4f342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:56:03.585012Z",
     "iopub.status.busy": "2025-04-19T15:56:03.584493Z",
     "iopub.status.idle": "2025-04-19T15:56:03.589765Z",
     "shell.execute_reply": "2025-04-19T15:56:03.589058Z",
     "shell.execute_reply.started": "2025-04-19T15:56:03.584975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>')\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "clf = LinearSVC()\n",
    "\n",
    "def data_preprocess(review):\n",
    "    review = re.sub(CLEANR, '', review)\n",
    "    review = re.sub('[^a-zA-Z ]', '', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(i) for i in review]\n",
    "    return ' '.join(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6372b5",
   "metadata": {},
   "source": [
    "### Loading the dataset and creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef317d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:56:09.537139Z",
     "iopub.status.busy": "2025-04-19T15:56:09.536290Z",
     "iopub.status.idle": "2025-04-19T15:56:50.424561Z",
     "shell.execute_reply": "2025-04-19T15:56:50.423929Z",
     "shell.execute_reply.started": "2025-04-19T15:56:09.537108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewer ha mentioned that af...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this wa a wonderful way to spend tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there a family where a little boy ja...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one of the other reviewer ha mentioned that af...          1\n",
       "1  a wonderful little production the filming tech...          1\n",
       "2  i thought this wa a wonderful way to spend tim...          1\n",
       "3  basically there a family where a little boy ja...          0\n",
       "4  petter matteis love in the time of money is a ...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "data['review'] = data['review'].apply(data_preprocess)\n",
    "data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef9854",
   "metadata": {},
   "source": [
    "### Splitting Data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b29fa58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:56:50.426120Z",
     "iopub.status.busy": "2025-04-19T15:56:50.425707Z",
     "iopub.status.idle": "2025-04-19T15:56:50.435497Z",
     "shell.execute_reply": "2025-04-19T15:56:50.434784Z",
     "shell.execute_reply.started": "2025-04-19T15:56:50.426101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f7723",
   "metadata": {},
   "source": [
    "### Baseline: TF-IDF + SVM\n",
    "We vectorize the text using TF-IDF and train a linear SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ce0d75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:56:50.436340Z",
     "iopub.status.busy": "2025-04-19T15:56:50.436162Z",
     "iopub.status.idle": "2025-04-19T15:56:57.893717Z",
     "shell.execute_reply": "2025-04-19T15:56:57.892948Z",
     "shell.execute_reply.started": "2025-04-19T15:56:50.436325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      5035\n",
      "           1       0.89      0.90      0.89      4965\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_vec = vectorizer.fit_transform(x_train)\n",
    "X_test_vec = vectorizer.transform(x_test)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b190b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:56:57.895927Z",
     "iopub.status.busy": "2025-04-19T15:56:57.895699Z",
     "iopub.status.idle": "2025-04-19T15:56:57.900622Z",
     "shell.execute_reply": "2025-04-19T15:56:57.900026Z",
     "shell.execute_reply.started": "2025-04-19T15:56:57.895908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.51.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67fa61",
   "metadata": {},
   "source": [
    "## Data Processing for Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb61b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:56:57.901518Z",
     "iopub.status.busy": "2025-04-19T15:56:57.901298Z",
     "iopub.status.idle": "2025-04-19T15:57:34.287924Z",
     "shell.execute_reply": "2025-04-19T15:57:34.287382Z",
     "shell.execute_reply.started": "2025-04-19T15:56:57.901494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_transformers = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "data_transformers['review'] = data_transformers['review'].apply(data_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ede8e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:57:34.288720Z",
     "iopub.status.busy": "2025-04-19T15:57:34.288543Z",
     "iopub.status.idle": "2025-04-19T15:57:34.303671Z",
     "shell.execute_reply": "2025-04-19T15:57:34.303039Z",
     "shell.execute_reply.started": "2025-04-19T15:57:34.288706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sentiments = list(data_transformers['sentiment'])\n",
    "labels = pd.get_dummies(sentiments)['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d50b638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:57:34.304520Z",
     "iopub.status.busy": "2025-04-19T15:57:34.304312Z",
     "iopub.status.idle": "2025-04-19T15:57:34.337253Z",
     "shell.execute_reply": "2025-04-19T15:57:34.336738Z",
     "shell.execute_reply.started": "2025-04-19T15:57:34.304496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(list(data_transformers['review']), labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "987367cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:57:34.338239Z",
     "iopub.status.busy": "2025-04-19T15:57:34.337973Z",
     "iopub.status.idle": "2025-04-19T15:57:36.141199Z",
     "shell.execute_reply": "2025-04-19T15:57:36.140637Z",
     "shell.execute_reply.started": "2025-04-19T15:57:34.338216Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22024e0c709840a3976b2226627cb7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27429204d2cc4388a764857756a22936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a767575c135b4b2fa272d338393ed81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b454c490ae64cbea5002579df6c3601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de803c",
   "metadata": {},
   "source": [
    "### Splitting and tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "294f6410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:57:36.142131Z",
     "iopub.status.busy": "2025-04-19T15:57:36.141862Z",
     "iopub.status.idle": "2025-04-19T15:58:48.394193Z",
     "shell.execute_reply": "2025-04-19T15:58:48.393628Z",
     "shell.execute_reply.started": "2025-04-19T15:57:36.142108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745078296.016767      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(x_train,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "test_encodings = tokenizer(x_test,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                                dict(train_encodings),\n",
    "                                y_train\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                                dict(test_encodings),\n",
    "                                y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e41cdd6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:58:48.396630Z",
     "iopub.status.busy": "2025-04-19T15:58:48.396249Z",
     "iopub.status.idle": "2025-04-19T15:58:50.401846Z",
     "shell.execute_reply": "2025-04-19T15:58:50.401274Z",
     "shell.execute_reply.started": "2025-04-19T15:58:48.396612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c1c186658b422ea9c35a48ea885bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d8214f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:58:50.402755Z",
     "iopub.status.busy": "2025-04-19T15:58:50.402562Z",
     "iopub.status.idle": "2025-04-19T15:58:51.868301Z",
     "shell.execute_reply": "2025-04-19T15:58:51.867635Z",
     "shell.execute_reply.started": "2025-04-19T15:58:50.402740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.04265959, 0.07845549]], dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset.take(1):\n",
    "    inputs, labels = data\n",
    "    predictions = model(inputs)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ffdd6",
   "metadata": {},
   "source": [
    "### Training for 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da66b417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T15:58:51.869246Z",
     "iopub.status.busy": "2025-04-19T15:58:51.868984Z",
     "iopub.status.idle": "2025-04-19T16:46:57.798771Z",
     "shell.execute_reply": "2025-04-19T16:46:57.797985Z",
     "shell.execute_reply.started": "2025-04-19T15:58:51.869220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745078354.481066     103 service.cc:148] XLA service 0x7f44b818d980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745078354.481548     103 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1745078354.552973     103 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1745078354.688759     103 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 1458s 572ms/step - loss: 0.7209 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.4965\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 1428s 571ms/step - loss: 0.6931 - accuracy: 0.5009 - val_loss: 0.6931 - val_accuracy: 0.4965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f460fdb5d90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_dataset.shuffle(100).batch(16),\n",
    "          epochs=2,\n",
    "          validation_data=test_dataset.shuffle(100).batch(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e505a9",
   "metadata": {},
   "source": [
    "### Saving Model check points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38055b26-931f-44cb-aaab-2bdabcf136dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:46:57.799918Z",
     "iopub.status.busy": "2025-04-19T16:46:57.799673Z",
     "iopub.status.idle": "2025-04-19T16:46:58.594723Z",
     "shell.execute_reply": "2025-04-19T16:46:58.594196Z",
     "shell.execute_reply.started": "2025-04-19T16:46:57.799896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./sentiment_transformer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51317df2-24a4-4d88-a76f-7bacd9ccd0bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:46:58.596452Z",
     "iopub.status.busy": "2025-04-19T16:46:58.596241Z",
     "iopub.status.idle": "2025-04-19T16:46:59.259870Z",
     "shell.execute_reply": "2025-04-19T16:46:59.259306Z",
     "shell.execute_reply.started": "2025-04-19T16:46:58.596436Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./sentiment_transformer_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./sentiment_transformer_model and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "prediction_model = TFDistilBertForSequenceClassification.from_pretrained(\"./sentiment_transformer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e374d47-0238-499f-8b4f-00532c4b5efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:46:59.260890Z",
     "iopub.status.busy": "2025-04-19T16:46:59.260650Z",
     "iopub.status.idle": "2025-04-19T16:46:59.265624Z",
     "shell.execute_reply": "2025-04-19T16:46:59.264909Z",
     "shell.execute_reply.started": "2025-04-19T16:46:59.260864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie is simply awesome it is so hilarious although the skating and other montage are played out the comedy is awesome raab himself and brandon dicamillo are hilarious there will be moment when you cant breath youre laughing so hard plus there are scene that you can watch hundred of time and still laugh this is one of the funniest comedy ive ever seen'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = x_test[4]\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34255e",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7e2fe20-e8a7-4f65-89be-6ab58f69e1eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:48:25.242568Z",
     "iopub.status.busy": "2025-04-19T16:48:25.242296Z",
     "iopub.status.idle": "2025-04-19T16:48:28.115461Z",
     "shell.execute_reply": "2025-04-19T16:48:28.114899Z",
     "shell.execute_reply.started": "2025-04-19T16:48:25.242548Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    }
   ],
   "source": [
    "predict_input = tokenizer.encode(test_sentence,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\")\n",
    "tf_output = prediction_model.predict(predict_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e9cd91a-11b7-4227-914b-d3eadf0de170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T16:48:28.806207Z",
     "iopub.status.busy": "2025-04-19T16:48:28.805418Z",
     "iopub.status.idle": "2025-04-19T16:48:28.811979Z",
     "shell.execute_reply": "2025-04-19T16:48:28.811225Z",
     "shell.execute_reply.started": "2025-04-19T16:48:28.806176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06466941, 0.9353306 ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_prediction = tf.nn.softmax(tf_output, axis=1).numpy()[0]\n",
    "tf_prediction"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
